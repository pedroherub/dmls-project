
@misc{ma_when_2020,
	title = {When {Federated} {Learning} {Meets} {Blockchain}: {A} {New} {Distributed} {Learning} {Paradigm}},
	shorttitle = {When {Federated} {Learning} {Meets} {Blockchain}},
	url = {http://arxiv.org/abs/2009.09338},
	doi = {10.48550/arXiv.2009.09338},
	abstract = {Motivated by the explosive computing capabilities at end user equipments, as well as the growing privacy concerns over sharing sensitive raw data, a new machine learning paradigm, named federated learning (FL) has emerged. By training models locally at each client and aggregating learning models at a central server, FL has the capability to avoid sharing data directly, thereby reducing privacy leakage. However, the traditional FL framework heavily relies on a single central server and may fall apart if such a server behaves maliciously. To address this single point of failure issue, this work investigates a blockchain assisted decentralized FL (BLADE-FL) framework, which can well prevent the malicious clients from poisoning the learning process, and further provides a self-motivated and reliable learning environment for clients. In detail, the model aggregation process is fully decentralized and the tasks of training for FL and mining for blockchain are integrated into each participant. In addition, we investigate the unique issues in this framework and provide analytical and experimental results to shed light on possible solutions.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Ma, Chuan and Li, Jun and Ding, Ming and Shi, Long and Wang, Taotao and Han, Zhu and Poor, H. Vincent},
	month = sep,
	year = {2020},
	note = {arXiv:2009.09338 [cs]
version: 1},
	keywords = {Computer Science - Networking and Internet Architecture},
}

@misc{kairouz_advances_2021,
	title = {Advances and {Open} {Problems} in {Federated} {Learning}},
	url = {http://arxiv.org/abs/1912.04977},
	doi = {10.48550/arXiv.1912.04977},
	abstract = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aurélien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D'Oliveira, Rafael G. L. and Eichner, Hubert and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gascón, Adrià and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Konečný, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancrède and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and Özgür, Ayfer and Pagh, Rasmus and Raykova, Mariana and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tramèr, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
	month = mar,
	year = {2021},
	note = {arXiv:1912.04977 [cs, stat]
version: 3},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wang_blockchain-based_2021,
	title = {Blockchain-based {Federated} {Learning}: {A} {Comprehensive} {Survey}},
	shorttitle = {Blockchain-based {Federated} {Learning}},
	url = {http://arxiv.org/abs/2110.02182},
	doi = {10.48550/arXiv.2110.02182},
	abstract = {With the technological advances in machine learning, effective ways are available to process the huge amount of data generated in real life. However, issues of privacy and scalability will constrain the development of machine learning. Federated learning (FL) can prevent privacy leakage by assigning training tasks to multiple clients, thus separating the central server from the local devices. However, FL still suffers from shortcomings such as single-point-failure and malicious data. The emergence of blockchain provides a secure and efficient solution for the deployment of FL. In this paper, we conduct a comprehensive survey of the literature on blockchained FL (BCFL). First, we investigate how blockchain can be applied to federal learning from the perspective of system composition. Then, we analyze the concrete functions of BCFL from the perspective of mechanism design and illustrate what problems blockchain addresses specifically for FL. We also survey the applications of BCFL in reality. Finally, we discuss some challenges and future research directions.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Wang, Zhilin and Hu, Qin},
	month = oct,
	year = {2021},
	note = {arXiv:2110.02182 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@article{kim_blockchained_2020,
	title = {Blockchained {On}-{Device} {Federated} {Learning}},
	volume = {24},
	issn = {1558-2558},
	doi = {10.1109/LCOMM.2019.2921755},
	abstract = {By leveraging blockchain, this letter proposes a blockchained federated learning (BlockFL) architecture where local learning model updates are exchanged and verified. This enables on-device machine learning without any centralized training data or coordination by utilizing a consensus mechanism in blockchain. Moreover, we analyze an end-to-end latency model of BlockFL and characterize the optimal block generation rate by considering communication, computation, and consensus delays.},
	number = {6},
	journal = {IEEE Communications Letters},
	author = {Kim, Hyesung and Park, Jihong and Bennis, Mehdi and Kim, Seong-Lyun},
	month = jun,
	year = {2020},
	keywords = {Blockchain, Computational modeling, Data models, Delays, Nickel, On-device machine learning, Servers, Training, blockchain, federated learning, latency},
	pages = {1279--1283},
}

@inproceedings{ur_rehman_towards_2020,
	title = {Towards {Blockchain}-{Based} {Reputation}-{Aware} {Federated} {Learning}},
	doi = {10.1109/INFOCOMWKSHPS50562.2020.9163027},
	abstract = {Federated learning (FL) is the collaborative machine learning (ML) technique whereby the devices collectively train and update a shared ML model while preserving their personal datasets. FL systems solve the problems of communication-efficiency, bandwidth-optimization, and privacy-preservation. Despite the potential benefits of FL, one centralized shared ML model across all the devices produce coarse-grained predictions which, in essence, are not required in many application areas involving personalized prediction services. In this paper, we present a novel concept of fine-grained FL to decentralize the shared ML models on the edge servers. We then present a formal extended definition of fine-grained FL process in mobile edge computing systems. In addition, we define the core requirements of fine-grained FL systems including personalization, decentralization, fine-grained FL, incentive mechanisms, trust, activity monitoring, heterogeneity and context-awareness, model synchronization, and communication and bandwidth-efficiency. Moreover, we present the concept of blockchain-based reputation-aware fine-grained FL in order to ensure trustworthy collaborative training in mobile edge computing systems. Finally, we perform the qualitative comparison of proposed approach with state-of-the-art related work and found some promising initial results.},
	booktitle = {{IEEE} {INFOCOM} 2020 - {IEEE} {Conference} on {Computer} {Communications} {Workshops} ({INFOCOM} {WKSHPS})},
	author = {ur Rehman, Muhammad Habib and Salah, Khaled and Damiani, Ernesto and Svetinovic, Davor},
	month = jul,
	year = {2020},
	keywords = {Cloud computing, Data models, Machine learning, Privacy, Servers, Training, blockchain, federated learning, machine learning, mobile edge computing, reputation, trust},
	pages = {183--188},
}

@article{toyoda_blockchain-enabled_2020,
	title = {Blockchain-{Enabled} {Federated} {Learning} {With} {Mechanism} {Design}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3043037},
	abstract = {Federated learning (FL) is a promising decentralized deep learning technique that allows users to collaboratively update models without sharing their own data. However, due to its decentralized nature, no one can monitor workers' behavior, and they may thus deviate protocols (e.g., participating without updating any models). To solve this problem, many researchers have proposed blockchain-enabled FL to reward workers (or users) with cryptocurrencies to encourage workers to follow the protocols. However, there is a lack of theoretical discussions concerning how such rewards impact workers' behavior and how much should be given to workers. In this article, we propose a mechanism-design-oriented FL protocol on a public blockchain network. Mechanism design (MD) is often used to make a rule intended to achieve a specific goal. With MD in mind, we introduce the concept of competition into blockchain-based FL so that only workers who have contributed well can obtain rewards, which naturally prevents workers from deviating from the protocol. We then mathematically answer the following questions with contest theory, a novel field of study in economics: i) What behavior will workers take?; ii) how much effort should workers exert to maximize their profits?; iii) how many workers should be rewarded?; and iv) what is the best proportion for reward distribution?},
	journal = {IEEE Access},
	author = {Toyoda, Kentaroh and Zhao, Jun and Zhang, Allan Neng Sheng and Mathiopoulos, P. Takis},
	year = {2020},
	keywords = {Biological system modeling, Blockchain, Cryptography, Data models, Federated learning, Protocols, Smart contracts, Task analysis, blockchain, contest theory, decentralized deep learning, mechanism design},
	pages = {219744--219756},
}

@inproceedings{toyoda_mechanism_2019,
	title = {Mechanism {Design} for {An} {Incentive}-aware {Blockchain}-enabled {Federated} {Learning} {Platform}},
	doi = {10.1109/BigData47090.2019.9006344},
	abstract = {Recent technological evolution enables Artificial Intelligence (AI) model training by users' mobile devices, which accelerates decentralized big data analysis. In particular, Federated Learning (FL) is a key enabler to realize decentralized AI model update without user's privacy disclosure. However, since the behaviour of workers, who are assigned a training task, cannot be monitored, the state-of-the-art methods require a special hardware and/or cryptography to force the workers behave honestly, which hinders the realization. Furthermore, although blockchain-enabled FL has been proposed to give workers reward, any rigorous reward policy design has not been discussed. In this paper, to tackle these issues, we present a novel method using mechanism design, which is an economic approach to realize desired objectives under the situation that participants act rationally. The key idea is to introduce repeated competition for FL so that any rational worker follows the protocol and maximize their profits. With mechanism design, we propose a generic full-fledged protocol design for FL on a public blockchain. We also theoretically clarify incentive compatibility based on contest theory which is an auction-based game theory in economics.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Toyoda, Kentaroh and Zhang, Allan N.},
	month = dec,
	year = {2019},
	keywords = {Bitcoin, Computational modeling, Contracts, Data models, Machine learning},
	pages = {395--403},
}

@inproceedings{martinez_record_2019,
	title = {Record and {Reward} {Federated} {Learning} {Contributions} with {Blockchain}},
	doi = {10.1109/CyberC.2019.00018},
	abstract = {Although Federated Learning allows for participants to contribute their local data without it being revealed, it faces issues in data security and in accurately paying participants for quality data contributions. In this paper, we propose an EOS Blockchain design and workflow to establish data security, a novel validation error based metric upon which we qualify gradient uploads for payment, and implement a small example of our blockchain Federated Learning model to analyze its performance.},
	booktitle = {2019 {International} {Conference} on {Cyber}-{Enabled} {Distributed} {Computing} and {Knowledge} {Discovery} ({CyberC})},
	author = {Martinez, Ismael and Francis, Sreya and Hafid, Abdelhakim Senhaji},
	month = oct,
	year = {2019},
	keywords = {Blockchain, Data models, Distributed Machine Learning, Distributed databases, Earth Observing System, Federated Learning, Machine learning, Smart contracts, Training, blockchain, class sampled validation error},
	pages = {50--57},
}

@inproceedings{ma_transparent_2021,
	title = {Transparent {Contribution} {Evaluation} for {Secure} {Federated} {Learning} on {Blockchain}},
	doi = {10.1109/ICDEW53142.2021.00023},
	abstract = {Federated Learning is a promising machine learning paradigm when multiple parties collaborate to build a high-quality machine learning model. Nonetheless, these parties are only willing to participate when given enough incentives, such as a fair reward based on their contributions. Many studies explored Shapley value based methods to evaluate each party's contribution to the learned model. However, they commonly assume a semi-trusted server to train the model and evaluate the data owners' model contributions, which lacks transparency and may hinder the success of federated learning in practice. In this work, we propose a blockchain-based federated learning framework and a protocol to transparently evaluate each participant's contribution. Our framework protects all parties' privacy in the model building phase and transparently evaluates contributions based on the model updates. The experiment with the handwritten digits dataset demonstrates that the proposed method can effectively evaluate the contributions.},
	booktitle = {2021 {IEEE} 37th {International} {Conference} on {Data} {Engineering} {Workshops} ({ICDEW})},
	author = {Ma, Shuaicheng and Cao, Yang and Xiong, Li},
	month = apr,
	year = {2021},
	note = {ISSN: 2473-3490},
	keywords = {Blockchain, Buildings, Collaborative work, Conferences, Contribution Evaluation, Federated Learning, Machine learning, Privacy, Protocols, Transparency},
	pages = {88--91},
}

@inproceedings{hou_systematic_2021,
	title = {A {Systematic} {Literature} {Review} of {Blockchain}-based {Federated} {Learning}: {Architectures}, {Applications} and {Issues}},
	shorttitle = {A {Systematic} {Literature} {Review} of {Blockchain}-based {Federated} {Learning}},
	doi = {10.1109/ICTC51749.2021.9441499},
	abstract = {Federal learning (FL) can realize a distributed training machine learning models in multiple devices while protecting their data privacy, but some defect still exists such as single point failure and lack of motivation. Blockchain as a distributed ledger can be utilized to provide a novel FL framework to address those issues. This paper aims to discuss how the blockchain technology is employed to compensate for shortcomings in FL. A systematic literature review is conducted to investigate existing FL problems and to summarize knowledge about the existing Blockchain-based FL (BFL). The differences among these collected BFL architectures are presented and discussed, and the applications of BFL are categorized and analyzed. Finally, some suggestions for future development and application of BFL are discussed.},
	booktitle = {2021 2nd {Information} {Communication} {Technologies} {Conference} ({ICTC})},
	author = {Hou, Dongkun and Zhang, Jie and Man, Ka Lok and Ma, Jieming and Peng, Zitian},
	month = may,
	year = {2021},
	keywords = {Bibliographies, Blockchain, Data privacy, Distributed ledger, Federated Learning, Machine learning, Systematic Literature Review, Systematics, Training},
	pages = {302--307},
}

@inproceedings{bao_flchain_2019,
	address = {QingDao, China},
	title = {{FLChain}: {A} {Blockchain} for {Auditable} {Federated} {Learning} with {Trust} and {Incentive}},
	isbn = {9781728140247},
	shorttitle = {{FLChain}},
	url = {https://ieeexplore.ieee.org/document/8905038/},
	doi = {10.1109/BIGCOM.2019.00030},
	urldate = {2022-10-19},
	booktitle = {2019 5th {International} {Conference} on {Big} {Data} {Computing} and {Communications} ({BIGCOM})},
	publisher = {IEEE},
	author = {Bao, Xianglin and Su, Cheng and Xiong, Yan and Huang, Wenchao and Hu, Yifei},
	month = aug,
	year = {2019},
	pages = {151--159},
}

@article{ali_integration_2021,
	title = {Integration of blockchain and federated learning for {Internet} of {Things}: {Recent} advances and future challenges},
	volume = {108},
	issn = {01674048},
	shorttitle = {Integration of blockchain and federated learning for {Internet} of {Things}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167404821001796},
	doi = {10.1016/j.cose.2021.102355},
	language = {en},
	urldate = {2022-10-19},
	journal = {Computers \& Security},
	author = {Ali, Mansoor and Karimipour, Hadis and Tariq, Muhammad},
	month = sep,
	year = {2021},
	pages = {102355},
}

@misc{ratner_mlsys_2019,
	title = {{MLSys}: {The} {New} {Frontier} of {Machine} {Learning} {Systems}},
	shorttitle = {{MLSys}},
	url = {http://arxiv.org/abs/1904.03257},
	abstract = {Machine learning (ML) techniques are enjoying rapidly increasing adoption. However, designing and implementing the systems that support ML models in real-world deployments remains a signiﬁcant obstacle, in large part due to the radically different development and deployment proﬁle of modern ML methods, and the range of practical concerns that come with broader adoption. We propose to foster a new systems machine learning research community at the intersection of the traditional systems and ML communities, focused on topics such as hardware systems for ML, software systems for ML, and ML optimized for metrics beyond predictive accuracy. To do this, we describe a new conference, MLSys, that explicitly targets research at the intersection of systems and machine learning with a program committee split evenly between experts in systems and ML, and an explicit focus on topics at the intersection of the two.},
	language = {en},
	urldate = {2022-09-30},
	publisher = {arXiv},
	author = {Ratner, Alexander and Alistarh, Dan and Alonso, Gustavo and Andersen, David G. and Bailis, Peter and Bird, Sarah and Carlini, Nicholas and Catanzaro, Bryan and Chayes, Jennifer and Chung, Eric and Dally, Bill and Dean, Jeff and Dhillon, Inderjit S. and Dimakis, Alexandros and Dubey, Pradeep and Elkan, Charles and Fursin, Grigori and Ganger, Gregory R. and Getoor, Lise and Gibbons, Phillip B. and Gibson, Garth A. and Gonzalez, Joseph E. and Gottschlich, Justin and Han, Song and Hazelwood, Kim and Huang, Furong and Jaggi, Martin and Jamieson, Kevin and Jordan, Michael I. and Joshi, Gauri and Khalaf, Rania and Knight, Jason and Konečný, Jakub and Kraska, Tim and Kumar, Arun and Kyrillidis, Anastasios and Lakshmiratan, Aparna and Li, Jing and Madden, Samuel and McMahan, H. Brendan and Meijer, Erik and Mitliagkas, Ioannis and Monga, Rajat and Murray, Derek and Olukotun, Kunle and Papailiopoulos, Dimitris and Pekhimenko, Gennady and Rekatsinas, Theodoros and Rostamizadeh, Afshin and Ré, Christopher and De Sa, Christopher and Sedghi, Hanie and Sen, Siddhartha and Smith, Virginia and Smola, Alex and Song, Dawn and Sparks, Evan and Stoica, Ion and Sze, Vivienne and Udell, Madeleine and Vanschoren, Joaquin and Venkataraman, Shivaram and Vinayak, Rashmi and Weimer, Markus and Wilson, Andrew Gordon and Xing, Eric and Zaharia, Matei and Zhang, Ce and Talwalkar, Ameet},
	month = dec,
	year = {2019},
	note = {arXiv:1904.03257 [cs, stat]},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Software Engineering, Statistics - Machine Learning},
}

@article{nguyen_federated_2021,
	title = {Federated {Learning} {Meets} {Blockchain} in {Edge} {Computing}: {Opportunities} and {Challenges}},
	volume = {8},
	issn = {2327-4662, 2372-2541},
	shorttitle = {Federated {Learning} {Meets} {Blockchain} in {Edge} {Computing}},
	url = {https://ieeexplore.ieee.org/document/9403374/},
	doi = {10.1109/JIOT.2021.3072611},
	number = {16},
	urldate = {2022-10-17},
	journal = {IEEE Internet of Things Journal},
	author = {Nguyen, Dinh C. and Ding, Ming and Pham, Quoc-Viet and Pathirana, Pubudu N. and Le, Long Bao and Seneviratne, Aruna and Li, Jun and Niyato, Dusit and Poor, H. Vincent},
	month = aug,
	year = {2021},
	pages = {12806--12825},
}
